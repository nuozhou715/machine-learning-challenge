{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in d:\\anaconda\\lib\\site-packages (from sklearn) (0.21.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Yanuo Zhou\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# Update sklearn to prevent version mismatches\n",
    "!pip install sklearn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "# install joblib. This will be used to save your model. \n",
    "# Restart your kernel after installing \n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_disposition</th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_period_err1</th>\n",
       "      <th>koi_period_err2</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_time0bk_err1</th>\n",
       "      <th>...</th>\n",
       "      <th>koi_steff_err2</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_slogg_err1</th>\n",
       "      <th>koi_slogg_err2</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>koi_srad_err1</th>\n",
       "      <th>koi_srad_err2</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.418383</td>\n",
       "      <td>2.479000e-04</td>\n",
       "      <td>-2.479000e-04</td>\n",
       "      <td>162.513840</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>...</td>\n",
       "      <td>-81</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.899140</td>\n",
       "      <td>1.490000e-05</td>\n",
       "      <td>-1.490000e-05</td>\n",
       "      <td>175.850252</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>...</td>\n",
       "      <td>-176</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>297.00482</td>\n",
       "      <td>48.134129</td>\n",
       "      <td>15.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.736952</td>\n",
       "      <td>2.630000e-07</td>\n",
       "      <td>-2.630000e-07</td>\n",
       "      <td>170.307565</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>...</td>\n",
       "      <td>-174</td>\n",
       "      <td>4.564</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>285.53461</td>\n",
       "      <td>48.285210</td>\n",
       "      <td>15.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.525592</td>\n",
       "      <td>3.760000e-06</td>\n",
       "      <td>-3.760000e-06</td>\n",
       "      <td>171.595550</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>...</td>\n",
       "      <td>-211</td>\n",
       "      <td>4.438</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>288.75488</td>\n",
       "      <td>48.226200</td>\n",
       "      <td>15.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.134435</td>\n",
       "      <td>1.050000e-05</td>\n",
       "      <td>-1.050000e-05</td>\n",
       "      <td>172.979370</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>...</td>\n",
       "      <td>-232</td>\n",
       "      <td>4.486</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>296.28613</td>\n",
       "      <td>48.224670</td>\n",
       "      <td>15.714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  koi_disposition  koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  \\\n",
       "0       CONFIRMED              0              0              0              0   \n",
       "1  FALSE POSITIVE              0              1              0              0   \n",
       "2  FALSE POSITIVE              0              1              0              0   \n",
       "3       CONFIRMED              0              0              0              0   \n",
       "4       CONFIRMED              0              0              0              0   \n",
       "\n",
       "   koi_period  koi_period_err1  koi_period_err2  koi_time0bk  \\\n",
       "0   54.418383     2.479000e-04    -2.479000e-04   162.513840   \n",
       "1   19.899140     1.490000e-05    -1.490000e-05   175.850252   \n",
       "2    1.736952     2.630000e-07    -2.630000e-07   170.307565   \n",
       "3    2.525592     3.760000e-06    -3.760000e-06   171.595550   \n",
       "4    4.134435     1.050000e-05    -1.050000e-05   172.979370   \n",
       "\n",
       "   koi_time0bk_err1  ...  koi_steff_err2  koi_slogg  koi_slogg_err1  \\\n",
       "0          0.003520  ...             -81      4.467           0.064   \n",
       "1          0.000581  ...            -176      4.544           0.044   \n",
       "2          0.000115  ...            -174      4.564           0.053   \n",
       "3          0.001130  ...            -211      4.438           0.070   \n",
       "4          0.001900  ...            -232      4.486           0.054   \n",
       "\n",
       "   koi_slogg_err2  koi_srad  koi_srad_err1  koi_srad_err2         ra  \\\n",
       "0          -0.096     0.927          0.105         -0.061  291.93423   \n",
       "1          -0.176     0.868          0.233         -0.078  297.00482   \n",
       "2          -0.168     0.791          0.201         -0.067  285.53461   \n",
       "3          -0.210     1.046          0.334         -0.133  288.75488   \n",
       "4          -0.229     0.972          0.315         -0.105  296.28613   \n",
       "\n",
       "         dec  koi_kepmag  \n",
       "0  48.141651      15.347  \n",
       "1  48.134129      15.436  \n",
       "2  48.285210      15.597  \n",
       "3  48.226200      15.509  \n",
       "4  48.224670      15.714  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"exoplanet_data.csv\")\n",
    "# Drop the null columns where all values are null\n",
    "df = df.dropna(axis='columns', how='all')\n",
    "# Drop the null rows\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select your features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6991, 37)\n"
     ]
    }
   ],
   "source": [
    "# Set features. This will also be used as your x values.\n",
    "# Only drop those that will cause colinearity issues, as we do not know which variable can be useful. Just throw everything\n",
    "# into the algorithms\n",
    "X = df.drop(['koi_period_err2', 'koi_time0bk_err2', 'koi_depth_err2', 'koi_disposition'], axis = 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Train Test Split\n",
    "\n",
    "Use `koi_disposition` for the y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['koi_disposition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Scale the data using the MinMaxScaler and perform some feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 5243 samples\n",
      "Epoch 1/100\n",
      "5243/5243 - 0s - loss: 0.9923 - acc: 0.6103\n",
      "Epoch 2/100\n",
      "5243/5243 - 0s - loss: 0.6494 - acc: 0.7578\n",
      "Epoch 3/100\n",
      "5243/5243 - 0s - loss: 0.4656 - acc: 0.7814\n",
      "Epoch 4/100\n",
      "5243/5243 - 0s - loss: 0.4115 - acc: 0.7936\n",
      "Epoch 5/100\n",
      "5243/5243 - 0s - loss: 0.3930 - acc: 0.7957\n",
      "Epoch 6/100\n",
      "5243/5243 - 0s - loss: 0.3805 - acc: 0.8055\n",
      "Epoch 7/100\n",
      "5243/5243 - 0s - loss: 0.3725 - acc: 0.8098\n",
      "Epoch 8/100\n",
      "5243/5243 - 0s - loss: 0.3648 - acc: 0.8220\n",
      "Epoch 9/100\n",
      "5243/5243 - 0s - loss: 0.3592 - acc: 0.8211\n",
      "Epoch 10/100\n",
      "5243/5243 - 0s - loss: 0.3549 - acc: 0.8219\n",
      "Epoch 11/100\n",
      "5243/5243 - 0s - loss: 0.3507 - acc: 0.8264\n",
      "Epoch 12/100\n",
      "5243/5243 - 0s - loss: 0.3472 - acc: 0.8289\n",
      "Epoch 13/100\n",
      "5243/5243 - 0s - loss: 0.3443 - acc: 0.8331\n",
      "Epoch 14/100\n",
      "5243/5243 - 0s - loss: 0.3420 - acc: 0.8356\n",
      "Epoch 15/100\n",
      "5243/5243 - 0s - loss: 0.3387 - acc: 0.8352\n",
      "Epoch 16/100\n",
      "5243/5243 - 0s - loss: 0.3361 - acc: 0.8390\n",
      "Epoch 17/100\n",
      "5243/5243 - 0s - loss: 0.3341 - acc: 0.8394\n",
      "Epoch 18/100\n",
      "5243/5243 - 0s - loss: 0.3313 - acc: 0.8459\n",
      "Epoch 19/100\n",
      "5243/5243 - 0s - loss: 0.3298 - acc: 0.8463\n",
      "Epoch 20/100\n",
      "5243/5243 - 0s - loss: 0.3280 - acc: 0.8470\n",
      "Epoch 21/100\n",
      "5243/5243 - 0s - loss: 0.3251 - acc: 0.8470\n",
      "Epoch 22/100\n",
      "5243/5243 - 0s - loss: 0.3248 - acc: 0.8488\n",
      "Epoch 23/100\n",
      "5243/5243 - 0s - loss: 0.3228 - acc: 0.8472\n",
      "Epoch 24/100\n",
      "5243/5243 - 0s - loss: 0.3203 - acc: 0.8497\n",
      "Epoch 25/100\n",
      "5243/5243 - 0s - loss: 0.3182 - acc: 0.8529\n",
      "Epoch 26/100\n",
      "5243/5243 - 0s - loss: 0.3170 - acc: 0.8556\n",
      "Epoch 27/100\n",
      "5243/5243 - 0s - loss: 0.3155 - acc: 0.8528\n",
      "Epoch 28/100\n",
      "5243/5243 - 0s - loss: 0.3138 - acc: 0.8556\n",
      "Epoch 29/100\n",
      "5243/5243 - 0s - loss: 0.3120 - acc: 0.8554\n",
      "Epoch 30/100\n",
      "5243/5243 - 0s - loss: 0.3114 - acc: 0.8560\n",
      "Epoch 31/100\n",
      "5243/5243 - 0s - loss: 0.3089 - acc: 0.8602\n",
      "Epoch 32/100\n",
      "5243/5243 - 0s - loss: 0.3083 - acc: 0.8598\n",
      "Epoch 33/100\n",
      "5243/5243 - 0s - loss: 0.3078 - acc: 0.8606\n",
      "Epoch 34/100\n",
      "5243/5243 - 0s - loss: 0.3058 - acc: 0.8619\n",
      "Epoch 35/100\n",
      "5243/5243 - 0s - loss: 0.3058 - acc: 0.8629\n",
      "Epoch 36/100\n",
      "5243/5243 - 0s - loss: 0.3041 - acc: 0.8619\n",
      "Epoch 37/100\n",
      "5243/5243 - 0s - loss: 0.3029 - acc: 0.8621\n",
      "Epoch 38/100\n",
      "5243/5243 - 0s - loss: 0.3021 - acc: 0.8625\n",
      "Epoch 39/100\n",
      "5243/5243 - 0s - loss: 0.3013 - acc: 0.8623\n",
      "Epoch 40/100\n",
      "5243/5243 - 0s - loss: 0.3000 - acc: 0.8638\n",
      "Epoch 41/100\n",
      "5243/5243 - 0s - loss: 0.2979 - acc: 0.8648\n",
      "Epoch 42/100\n",
      "5243/5243 - 0s - loss: 0.2984 - acc: 0.8655\n",
      "Epoch 43/100\n",
      "5243/5243 - 0s - loss: 0.2976 - acc: 0.8688\n",
      "Epoch 44/100\n",
      "5243/5243 - 0s - loss: 0.2970 - acc: 0.8661\n",
      "Epoch 45/100\n",
      "5243/5243 - 0s - loss: 0.2954 - acc: 0.8693\n",
      "Epoch 46/100\n",
      "5243/5243 - 0s - loss: 0.2967 - acc: 0.8655\n",
      "Epoch 47/100\n",
      "5243/5243 - 0s - loss: 0.2952 - acc: 0.8673\n",
      "Epoch 48/100\n",
      "5243/5243 - 0s - loss: 0.2940 - acc: 0.8695\n",
      "Epoch 49/100\n",
      "5243/5243 - 0s - loss: 0.2941 - acc: 0.8718\n",
      "Epoch 50/100\n",
      "5243/5243 - 0s - loss: 0.2941 - acc: 0.8669\n",
      "Epoch 51/100\n",
      "5243/5243 - 0s - loss: 0.2931 - acc: 0.8686\n",
      "Epoch 52/100\n",
      "5243/5243 - 0s - loss: 0.2937 - acc: 0.8688\n",
      "Epoch 53/100\n",
      "5243/5243 - 0s - loss: 0.2913 - acc: 0.8734\n",
      "Epoch 54/100\n",
      "5243/5243 - 0s - loss: 0.2902 - acc: 0.8701\n",
      "Epoch 55/100\n",
      "5243/5243 - 0s - loss: 0.2906 - acc: 0.8741\n",
      "Epoch 56/100\n",
      "5243/5243 - 0s - loss: 0.2916 - acc: 0.8709\n",
      "Epoch 57/100\n",
      "5243/5243 - 0s - loss: 0.2913 - acc: 0.8722\n",
      "Epoch 58/100\n",
      "5243/5243 - 0s - loss: 0.2888 - acc: 0.8724\n",
      "Epoch 59/100\n",
      "5243/5243 - 0s - loss: 0.2906 - acc: 0.8734\n",
      "Epoch 60/100\n",
      "5243/5243 - 0s - loss: 0.2872 - acc: 0.8739\n",
      "Epoch 61/100\n",
      "5243/5243 - 0s - loss: 0.2857 - acc: 0.8762\n",
      "Epoch 62/100\n",
      "5243/5243 - 0s - loss: 0.2879 - acc: 0.8762\n",
      "Epoch 63/100\n",
      "5243/5243 - 0s - loss: 0.2871 - acc: 0.8774\n",
      "Epoch 64/100\n",
      "5243/5243 - 0s - loss: 0.2863 - acc: 0.8747\n",
      "Epoch 65/100\n",
      "5243/5243 - 0s - loss: 0.2872 - acc: 0.8760\n",
      "Epoch 66/100\n",
      "5243/5243 - 0s - loss: 0.2862 - acc: 0.8781\n",
      "Epoch 67/100\n",
      "5243/5243 - 0s - loss: 0.2865 - acc: 0.8749\n",
      "Epoch 68/100\n",
      "5243/5243 - 0s - loss: 0.2851 - acc: 0.8758\n",
      "Epoch 69/100\n",
      "5243/5243 - 0s - loss: 0.2841 - acc: 0.8779\n",
      "Epoch 70/100\n",
      "5243/5243 - 0s - loss: 0.2859 - acc: 0.8772\n",
      "Epoch 71/100\n",
      "5243/5243 - 0s - loss: 0.2834 - acc: 0.8777\n",
      "Epoch 72/100\n",
      "5243/5243 - 0s - loss: 0.2835 - acc: 0.8798\n",
      "Epoch 73/100\n",
      "5243/5243 - 0s - loss: 0.2853 - acc: 0.8779\n",
      "Epoch 74/100\n",
      "5243/5243 - 0s - loss: 0.2855 - acc: 0.8755\n",
      "Epoch 75/100\n",
      "5243/5243 - 0s - loss: 0.2820 - acc: 0.8796\n",
      "Epoch 76/100\n",
      "5243/5243 - 0s - loss: 0.2817 - acc: 0.8817\n",
      "Epoch 77/100\n",
      "5243/5243 - 0s - loss: 0.2816 - acc: 0.8806\n",
      "Epoch 78/100\n",
      "5243/5243 - 0s - loss: 0.2812 - acc: 0.8812\n",
      "Epoch 79/100\n",
      "5243/5243 - 0s - loss: 0.2809 - acc: 0.8812\n",
      "Epoch 80/100\n",
      "5243/5243 - 0s - loss: 0.2800 - acc: 0.8814\n",
      "Epoch 81/100\n",
      "5243/5243 - 0s - loss: 0.2824 - acc: 0.8787\n",
      "Epoch 82/100\n",
      "5243/5243 - 0s - loss: 0.2819 - acc: 0.8810\n",
      "Epoch 83/100\n",
      "5243/5243 - 0s - loss: 0.2804 - acc: 0.8854\n",
      "Epoch 84/100\n",
      "5243/5243 - 0s - loss: 0.2785 - acc: 0.8816\n",
      "Epoch 85/100\n",
      "5243/5243 - 0s - loss: 0.2798 - acc: 0.8858\n",
      "Epoch 86/100\n",
      "5243/5243 - 0s - loss: 0.2792 - acc: 0.8800\n",
      "Epoch 87/100\n",
      "5243/5243 - 0s - loss: 0.2796 - acc: 0.8819\n",
      "Epoch 88/100\n",
      "5243/5243 - 0s - loss: 0.2778 - acc: 0.8827\n",
      "Epoch 89/100\n",
      "5243/5243 - 0s - loss: 0.2804 - acc: 0.8833\n",
      "Epoch 90/100\n",
      "5243/5243 - 0s - loss: 0.2794 - acc: 0.8781\n",
      "Epoch 91/100\n",
      "5243/5243 - 0s - loss: 0.2795 - acc: 0.8831\n",
      "Epoch 92/100\n",
      "5243/5243 - 0s - loss: 0.2801 - acc: 0.8802\n",
      "Epoch 93/100\n",
      "5243/5243 - 0s - loss: 0.2799 - acc: 0.8823\n",
      "Epoch 94/100\n",
      "5243/5243 - 0s - loss: 0.2765 - acc: 0.8856\n",
      "Epoch 95/100\n",
      "5243/5243 - 0s - loss: 0.2779 - acc: 0.8835\n",
      "Epoch 96/100\n",
      "5243/5243 - 0s - loss: 0.2785 - acc: 0.8840\n",
      "Epoch 97/100\n",
      "5243/5243 - 0s - loss: 0.2772 - acc: 0.8842\n",
      "Epoch 98/100\n",
      "5243/5243 - 0s - loss: 0.2785 - acc: 0.8837\n",
      "Epoch 99/100\n",
      "5243/5243 - 0s - loss: 0.2789 - acc: 0.8844\n",
      "Epoch 100/100\n",
      "5243/5243 - 0s - loss: 0.2773 - acc: 0.8861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc6afc2eb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a Neural Network model here\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(units = 6, activation = 'relu', input_dim = X_train_scaled.shape[1]))\n",
    "model2.add(Dense(units = 4, activation = 'relu'))\n",
    "model2.add(Dense(units = y_train_categorical.shape[1], activation = 'softmax'))\n",
    "\n",
    "# Compile the model using categorical_crossentropy for the loss function, the adam optimizer,\n",
    "# and add accuracy to the training metrics\n",
    "model2.compile(optimizer='adam',\n",
    "               loss ='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# Use the training data to fit (train) the model\n",
    "model2.fit(X_train_scaled,\n",
    "           y_train_categorical,\n",
    "           epochs=100,\n",
    "           shuffle=True,\n",
    "           verbose = 2\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5243/5243 [==============================] - 0s 31us/sample - loss: 0.2757 - acc: 0.8892\n",
      "Training Data Score: 0.8891856074333191\n",
      "1748/1748 [==============================] - 0s 22us/sample - loss: 0.3002 - acc: 0.8793\n",
      "Testing Data Score: 0.8792906403541565\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {model2.evaluate(X_train_scaled, y_train_categorical)[1]}\")\n",
    "print(f\"Testing Data Score: {model2.evaluate(X_test_scaled, y_test_categorical)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Use `GridSearchCV` to tune the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5243 samples\n",
      "Epoch 1/150\n",
      "5243/5243 - 0s - loss: 1.0003 - acc: 0.5895\n",
      "Epoch 2/150\n",
      "5243/5243 - 0s - loss: 0.7468 - acc: 0.7265\n",
      "Epoch 3/150\n",
      "5243/5243 - 0s - loss: 0.5248 - acc: 0.7379\n",
      "Epoch 4/150\n",
      "5243/5243 - 0s - loss: 0.4294 - acc: 0.7372\n",
      "Epoch 5/150\n",
      "5243/5243 - 0s - loss: 0.4038 - acc: 0.7616\n",
      "Epoch 6/150\n",
      "5243/5243 - 0s - loss: 0.3948 - acc: 0.7658\n",
      "Epoch 7/150\n",
      "5243/5243 - 0s - loss: 0.3906 - acc: 0.7814\n",
      "Epoch 8/150\n",
      "5243/5243 - 0s - loss: 0.3887 - acc: 0.7700\n",
      "Epoch 9/150\n",
      "5243/5243 - 0s - loss: 0.3876 - acc: 0.7891\n",
      "Epoch 10/150\n",
      "5243/5243 - 0s - loss: 0.3848 - acc: 0.7873\n",
      "Epoch 11/150\n",
      "5243/5243 - 0s - loss: 0.3830 - acc: 0.7932\n",
      "Epoch 12/150\n",
      "5243/5243 - 0s - loss: 0.3821 - acc: 0.8051\n",
      "Epoch 13/150\n",
      "5243/5243 - 0s - loss: 0.3793 - acc: 0.8020\n",
      "Epoch 14/150\n",
      "5243/5243 - 0s - loss: 0.3757 - acc: 0.8123\n",
      "Epoch 15/150\n",
      "5243/5243 - 0s - loss: 0.3715 - acc: 0.8224\n",
      "Epoch 16/150\n",
      "5243/5243 - 0s - loss: 0.3668 - acc: 0.8249\n",
      "Epoch 17/150\n",
      "5243/5243 - 0s - loss: 0.3626 - acc: 0.8238\n",
      "Epoch 18/150\n",
      "5243/5243 - 0s - loss: 0.3590 - acc: 0.8278\n",
      "Epoch 19/150\n",
      "5243/5243 - 0s - loss: 0.3551 - acc: 0.8295\n",
      "Epoch 20/150\n",
      "5243/5243 - 0s - loss: 0.3513 - acc: 0.8350\n",
      "Epoch 21/150\n",
      "5243/5243 - 0s - loss: 0.3472 - acc: 0.8383\n",
      "Epoch 22/150\n",
      "5243/5243 - 0s - loss: 0.3449 - acc: 0.8346\n",
      "Epoch 23/150\n",
      "5243/5243 - 0s - loss: 0.3434 - acc: 0.8365\n",
      "Epoch 24/150\n",
      "5243/5243 - 0s - loss: 0.3407 - acc: 0.8343\n",
      "Epoch 25/150\n",
      "5243/5243 - 0s - loss: 0.3391 - acc: 0.8394\n",
      "Epoch 26/150\n",
      "5243/5243 - 0s - loss: 0.3376 - acc: 0.8426\n",
      "Epoch 27/150\n",
      "5243/5243 - 0s - loss: 0.3364 - acc: 0.8407\n",
      "Epoch 28/150\n",
      "5243/5243 - 0s - loss: 0.3330 - acc: 0.8447\n",
      "Epoch 29/150\n",
      "5243/5243 - 0s - loss: 0.3306 - acc: 0.8442\n",
      "Epoch 30/150\n",
      "5243/5243 - 0s - loss: 0.3272 - acc: 0.8470\n",
      "Epoch 31/150\n",
      "5243/5243 - 0s - loss: 0.3269 - acc: 0.8449\n",
      "Epoch 32/150\n",
      "5243/5243 - 0s - loss: 0.3230 - acc: 0.8510\n",
      "Epoch 33/150\n",
      "5243/5243 - 0s - loss: 0.3214 - acc: 0.8486\n",
      "Epoch 34/150\n",
      "5243/5243 - 0s - loss: 0.3186 - acc: 0.8516\n",
      "Epoch 35/150\n",
      "5243/5243 - 0s - loss: 0.3164 - acc: 0.8547\n",
      "Epoch 36/150\n",
      "5243/5243 - 0s - loss: 0.3158 - acc: 0.8522\n",
      "Epoch 37/150\n",
      "5243/5243 - 0s - loss: 0.3147 - acc: 0.8568\n",
      "Epoch 38/150\n",
      "5243/5243 - 0s - loss: 0.3129 - acc: 0.8547\n",
      "Epoch 39/150\n",
      "5243/5243 - 0s - loss: 0.3107 - acc: 0.8585\n",
      "Epoch 40/150\n",
      "5243/5243 - 0s - loss: 0.3085 - acc: 0.8623\n",
      "Epoch 41/150\n",
      "5243/5243 - 0s - loss: 0.3094 - acc: 0.8579\n",
      "Epoch 42/150\n",
      "5243/5243 - 0s - loss: 0.3067 - acc: 0.8619\n",
      "Epoch 43/150\n",
      "5243/5243 - 0s - loss: 0.3047 - acc: 0.8623\n",
      "Epoch 44/150\n",
      "5243/5243 - 0s - loss: 0.3049 - acc: 0.8621\n",
      "Epoch 45/150\n",
      "5243/5243 - 0s - loss: 0.3039 - acc: 0.8634\n",
      "Epoch 46/150\n",
      "5243/5243 - 0s - loss: 0.3025 - acc: 0.8655\n",
      "Epoch 47/150\n",
      "5243/5243 - 0s - loss: 0.3011 - acc: 0.8621\n",
      "Epoch 48/150\n",
      "5243/5243 - 0s - loss: 0.2993 - acc: 0.8671\n",
      "Epoch 49/150\n",
      "5243/5243 - 0s - loss: 0.3006 - acc: 0.8655\n",
      "Epoch 50/150\n",
      "5243/5243 - 0s - loss: 0.2968 - acc: 0.8650\n",
      "Epoch 51/150\n",
      "5243/5243 - 0s - loss: 0.2968 - acc: 0.8659\n",
      "Epoch 52/150\n",
      "5243/5243 - 0s - loss: 0.2964 - acc: 0.8652\n",
      "Epoch 53/150\n",
      "5243/5243 - 0s - loss: 0.2938 - acc: 0.8707\n",
      "Epoch 54/150\n",
      "5243/5243 - 0s - loss: 0.2935 - acc: 0.8709\n",
      "Epoch 55/150\n",
      "5243/5243 - 0s - loss: 0.2915 - acc: 0.8728\n",
      "Epoch 56/150\n",
      "5243/5243 - 0s - loss: 0.2916 - acc: 0.8703\n",
      "Epoch 57/150\n",
      "5243/5243 - 0s - loss: 0.2912 - acc: 0.8716\n",
      "Epoch 58/150\n",
      "5243/5243 - 0s - loss: 0.2897 - acc: 0.8735\n",
      "Epoch 59/150\n",
      "5243/5243 - 0s - loss: 0.2895 - acc: 0.8739\n",
      "Epoch 60/150\n",
      "5243/5243 - 0s - loss: 0.2887 - acc: 0.8741\n",
      "Epoch 61/150\n",
      "5243/5243 - 0s - loss: 0.2872 - acc: 0.8730\n",
      "Epoch 62/150\n",
      "5243/5243 - 0s - loss: 0.2872 - acc: 0.8766\n",
      "Epoch 63/150\n",
      "5243/5243 - 0s - loss: 0.2891 - acc: 0.8728\n",
      "Epoch 64/150\n",
      "5243/5243 - 0s - loss: 0.2848 - acc: 0.8800\n",
      "Epoch 65/150\n",
      "5243/5243 - 0s - loss: 0.2858 - acc: 0.8749\n",
      "Epoch 66/150\n",
      "5243/5243 - 0s - loss: 0.2858 - acc: 0.8777\n",
      "Epoch 67/150\n",
      "5243/5243 - 0s - loss: 0.2862 - acc: 0.8766\n",
      "Epoch 68/150\n",
      "5243/5243 - 0s - loss: 0.2842 - acc: 0.8756\n",
      "Epoch 69/150\n",
      "5243/5243 - 0s - loss: 0.2850 - acc: 0.8774\n",
      "Epoch 70/150\n",
      "5243/5243 - 0s - loss: 0.2828 - acc: 0.8783\n",
      "Epoch 71/150\n",
      "5243/5243 - 0s - loss: 0.2806 - acc: 0.8837\n",
      "Epoch 72/150\n",
      "5243/5243 - 0s - loss: 0.2800 - acc: 0.8791\n",
      "Epoch 73/150\n",
      "5243/5243 - 0s - loss: 0.2811 - acc: 0.8831\n",
      "Epoch 74/150\n",
      "5243/5243 - 0s - loss: 0.2796 - acc: 0.8838\n",
      "Epoch 75/150\n",
      "5243/5243 - 0s - loss: 0.2795 - acc: 0.8835\n",
      "Epoch 76/150\n",
      "5243/5243 - 0s - loss: 0.2797 - acc: 0.8829\n",
      "Epoch 77/150\n",
      "5243/5243 - 0s - loss: 0.2775 - acc: 0.8869\n",
      "Epoch 78/150\n",
      "5243/5243 - 0s - loss: 0.2788 - acc: 0.8837\n",
      "Epoch 79/150\n",
      "5243/5243 - 0s - loss: 0.2775 - acc: 0.8806\n",
      "Epoch 80/150\n",
      "5243/5243 - 0s - loss: 0.2757 - acc: 0.8861\n",
      "Epoch 81/150\n",
      "5243/5243 - 0s - loss: 0.2765 - acc: 0.8814\n",
      "Epoch 82/150\n",
      "5243/5243 - 0s - loss: 0.2744 - acc: 0.8880\n",
      "Epoch 83/150\n",
      "5243/5243 - 0s - loss: 0.2744 - acc: 0.8846\n",
      "Epoch 84/150\n",
      "5243/5243 - 0s - loss: 0.2795 - acc: 0.8827\n",
      "Epoch 85/150\n",
      "5243/5243 - 0s - loss: 0.2762 - acc: 0.8884\n",
      "Epoch 86/150\n",
      "5243/5243 - 0s - loss: 0.2760 - acc: 0.8808\n",
      "Epoch 87/150\n",
      "5243/5243 - 0s - loss: 0.2749 - acc: 0.8846\n",
      "Epoch 88/150\n",
      "5243/5243 - 0s - loss: 0.2729 - acc: 0.8869\n",
      "Epoch 89/150\n",
      "5243/5243 - 0s - loss: 0.2730 - acc: 0.8861\n",
      "Epoch 90/150\n",
      "5243/5243 - 0s - loss: 0.2717 - acc: 0.8869\n",
      "Epoch 91/150\n",
      "5243/5243 - 0s - loss: 0.2713 - acc: 0.8865\n",
      "Epoch 92/150\n",
      "5243/5243 - 0s - loss: 0.2717 - acc: 0.8879\n",
      "Epoch 93/150\n",
      "5243/5243 - 0s - loss: 0.2715 - acc: 0.8867\n",
      "Epoch 94/150\n",
      "5243/5243 - 0s - loss: 0.2703 - acc: 0.8892\n",
      "Epoch 95/150\n",
      "5243/5243 - 0s - loss: 0.2711 - acc: 0.8880\n",
      "Epoch 96/150\n",
      "5243/5243 - 0s - loss: 0.2746 - acc: 0.8850\n",
      "Epoch 97/150\n",
      "5243/5243 - 0s - loss: 0.2716 - acc: 0.8852\n",
      "Epoch 98/150\n",
      "5243/5243 - 0s - loss: 0.2707 - acc: 0.8867\n",
      "Epoch 99/150\n",
      "5243/5243 - 0s - loss: 0.2689 - acc: 0.8871\n",
      "Epoch 100/150\n",
      "5243/5243 - 0s - loss: 0.2704 - acc: 0.8903\n",
      "Epoch 101/150\n",
      "5243/5243 - 0s - loss: 0.2693 - acc: 0.8888\n",
      "Epoch 102/150\n",
      "5243/5243 - 0s - loss: 0.2692 - acc: 0.8901\n",
      "Epoch 103/150\n",
      "5243/5243 - 0s - loss: 0.2690 - acc: 0.8852\n",
      "Epoch 104/150\n",
      "5243/5243 - 0s - loss: 0.2693 - acc: 0.8899\n",
      "Epoch 105/150\n",
      "5243/5243 - 0s - loss: 0.2679 - acc: 0.8901\n",
      "Epoch 106/150\n",
      "5243/5243 - 0s - loss: 0.2680 - acc: 0.8888\n",
      "Epoch 107/150\n",
      "5243/5243 - 0s - loss: 0.2674 - acc: 0.8880\n",
      "Epoch 108/150\n",
      "5243/5243 - 0s - loss: 0.2661 - acc: 0.8903\n",
      "Epoch 109/150\n",
      "5243/5243 - 0s - loss: 0.2671 - acc: 0.8905\n",
      "Epoch 110/150\n",
      "5243/5243 - 0s - loss: 0.2666 - acc: 0.8880\n",
      "Epoch 111/150\n",
      "5243/5243 - 0s - loss: 0.2636 - acc: 0.8909\n",
      "Epoch 112/150\n",
      "5243/5243 - 0s - loss: 0.2643 - acc: 0.8922\n",
      "Epoch 113/150\n",
      "5243/5243 - 0s - loss: 0.2667 - acc: 0.8911\n",
      "Epoch 114/150\n",
      "5243/5243 - 0s - loss: 0.2634 - acc: 0.8936\n",
      "Epoch 115/150\n",
      "5243/5243 - 0s - loss: 0.2693 - acc: 0.8880\n",
      "Epoch 116/150\n",
      "5243/5243 - 0s - loss: 0.2645 - acc: 0.8907\n",
      "Epoch 117/150\n",
      "5243/5243 - 0s - loss: 0.2645 - acc: 0.8915\n",
      "Epoch 118/150\n",
      "5243/5243 - 0s - loss: 0.2642 - acc: 0.8920\n",
      "Epoch 119/150\n",
      "5243/5243 - 0s - loss: 0.2620 - acc: 0.8903\n",
      "Epoch 120/150\n",
      "5243/5243 - 0s - loss: 0.2625 - acc: 0.8928\n",
      "Epoch 121/150\n",
      "5243/5243 - 0s - loss: 0.2637 - acc: 0.8915\n",
      "Epoch 122/150\n",
      "5243/5243 - 0s - loss: 0.2630 - acc: 0.8907\n",
      "Epoch 123/150\n",
      "5243/5243 - 0s - loss: 0.2630 - acc: 0.8926\n",
      "Epoch 124/150\n",
      "5243/5243 - 0s - loss: 0.2611 - acc: 0.8932\n",
      "Epoch 125/150\n",
      "5243/5243 - 0s - loss: 0.2607 - acc: 0.8930\n",
      "Epoch 126/150\n",
      "5243/5243 - 0s - loss: 0.2643 - acc: 0.8907\n",
      "Epoch 127/150\n",
      "5243/5243 - 0s - loss: 0.2616 - acc: 0.8905\n",
      "Epoch 128/150\n",
      "5243/5243 - 0s - loss: 0.2612 - acc: 0.8917\n",
      "Epoch 129/150\n",
      "5243/5243 - 0s - loss: 0.2590 - acc: 0.8915\n",
      "Epoch 130/150\n",
      "5243/5243 - 0s - loss: 0.2603 - acc: 0.8920\n",
      "Epoch 131/150\n",
      "5243/5243 - 0s - loss: 0.2597 - acc: 0.8922\n",
      "Epoch 132/150\n",
      "5243/5243 - 0s - loss: 0.2598 - acc: 0.8924\n",
      "Epoch 133/150\n",
      "5243/5243 - 0s - loss: 0.2592 - acc: 0.8903\n",
      "Epoch 134/150\n",
      "5243/5243 - 0s - loss: 0.2574 - acc: 0.8941\n",
      "Epoch 135/150\n",
      "5243/5243 - 0s - loss: 0.2571 - acc: 0.8951\n",
      "Epoch 136/150\n",
      "5243/5243 - 0s - loss: 0.2598 - acc: 0.8882\n",
      "Epoch 137/150\n",
      "5243/5243 - 0s - loss: 0.2570 - acc: 0.8926\n",
      "Epoch 138/150\n",
      "5243/5243 - 0s - loss: 0.2558 - acc: 0.8943\n",
      "Epoch 139/150\n",
      "5243/5243 - 0s - loss: 0.2558 - acc: 0.8934\n",
      "Epoch 140/150\n",
      "5243/5243 - 0s - loss: 0.2568 - acc: 0.8926\n",
      "Epoch 141/150\n",
      "5243/5243 - 0s - loss: 0.2566 - acc: 0.8909\n",
      "Epoch 142/150\n",
      "5243/5243 - 0s - loss: 0.2542 - acc: 0.8936\n",
      "Epoch 143/150\n",
      "5243/5243 - 0s - loss: 0.2595 - acc: 0.8903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/150\n",
      "5243/5243 - 0s - loss: 0.2593 - acc: 0.8867\n",
      "Epoch 145/150\n",
      "5243/5243 - 0s - loss: 0.2520 - acc: 0.8961\n",
      "Epoch 146/150\n",
      "5243/5243 - 0s - loss: 0.2546 - acc: 0.8947\n",
      "Epoch 147/150\n",
      "5243/5243 - 0s - loss: 0.2530 - acc: 0.8951\n",
      "Epoch 148/150\n",
      "5243/5243 - 0s - loss: 0.2526 - acc: 0.8947\n",
      "Epoch 149/150\n",
      "5243/5243 - 0s - loss: 0.2528 - acc: 0.8949\n",
      "Epoch 150/150\n",
      "5243/5243 - 0s - loss: 0.2540 - acc: 0.8945\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons=6):\n",
    "# create model\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(units = neurons, activation = 'relu', input_dim = 37))\n",
    "    model2.add(Dense(units = 4, activation = 'relu'))\n",
    "    model2.add(Dense(units = 3, activation='softmax'))\n",
    "# Compile model\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model2\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model2 = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "# define the grid search parameters\n",
    "epochs = [50, 100, 150]\n",
    "neurons = [6, 8, 10]\n",
    "param_grid = dict(neurons = neurons, epochs = epochs)\n",
    "grid = GridSearchCV(estimator=model2, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled, y_train_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.878505 using {'epochs': 150, 'neurons': 8}\n",
      "0.724776 (0.158537) with: {'epochs': 50, 'neurons': 6}\n",
      "0.831394 (0.048773) with: {'epochs': 50, 'neurons': 8}\n",
      "0.830059 (0.047905) with: {'epochs': 50, 'neurons': 10}\n",
      "0.865916 (0.003819) with: {'epochs': 100, 'neurons': 6}\n",
      "0.876597 (0.006866) with: {'epochs': 100, 'neurons': 8}\n",
      "0.872973 (0.003719) with: {'epochs': 100, 'neurons': 10}\n",
      "0.856380 (0.013045) with: {'epochs': 150, 'neurons': 6}\n",
      "0.878505 (0.007162) with: {'epochs': 150, 'neurons': 8}\n",
      "0.872020 (0.002749) with: {'epochs': 150, 'neurons': 10}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Accuracy of the Tuned Model on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1748 samples\n",
      "Epoch 1/150\n",
      "1748/1748 - 0s - loss: 1.0659 - acc: 0.4199\n",
      "Epoch 2/150\n",
      "1748/1748 - 0s - loss: 1.0141 - acc: 0.5011\n",
      "Epoch 3/150\n",
      "1748/1748 - 0s - loss: 0.9709 - acc: 0.5017\n",
      "Epoch 4/150\n",
      "1748/1748 - 0s - loss: 0.9178 - acc: 0.5046\n",
      "Epoch 5/150\n",
      "1748/1748 - 0s - loss: 0.8457 - acc: 0.5280\n",
      "Epoch 6/150\n",
      "1748/1748 - 0s - loss: 0.7647 - acc: 0.5795\n",
      "Epoch 7/150\n",
      "1748/1748 - 0s - loss: 0.6932 - acc: 0.6808\n",
      "Epoch 8/150\n",
      "1748/1748 - 0s - loss: 0.6392 - acc: 0.7271\n",
      "Epoch 9/150\n",
      "1748/1748 - 0s - loss: 0.5990 - acc: 0.7391\n",
      "Epoch 10/150\n",
      "1748/1748 - 0s - loss: 0.5683 - acc: 0.7391\n",
      "Epoch 11/150\n",
      "1748/1748 - 0s - loss: 0.5451 - acc: 0.7746\n",
      "Epoch 12/150\n",
      "1748/1748 - 0s - loss: 0.5262 - acc: 0.7763\n",
      "Epoch 13/150\n",
      "1748/1748 - 0s - loss: 0.5094 - acc: 0.7769\n",
      "Epoch 14/150\n",
      "1748/1748 - 0s - loss: 0.4953 - acc: 0.7769\n",
      "Epoch 15/150\n",
      "1748/1748 - 0s - loss: 0.4841 - acc: 0.7929\n",
      "Epoch 16/150\n",
      "1748/1748 - 0s - loss: 0.4737 - acc: 0.7935\n",
      "Epoch 17/150\n",
      "1748/1748 - 0s - loss: 0.4635 - acc: 0.7992\n",
      "Epoch 18/150\n",
      "1748/1748 - 0s - loss: 0.4548 - acc: 0.7975\n",
      "Epoch 19/150\n",
      "1748/1748 - 0s - loss: 0.4473 - acc: 0.8084\n",
      "Epoch 20/150\n",
      "1748/1748 - 0s - loss: 0.4406 - acc: 0.8124\n",
      "Epoch 21/150\n",
      "1748/1748 - 0s - loss: 0.4332 - acc: 0.8106\n",
      "Epoch 22/150\n",
      "1748/1748 - 0s - loss: 0.4268 - acc: 0.8112\n",
      "Epoch 23/150\n",
      "1748/1748 - 0s - loss: 0.4208 - acc: 0.8215\n",
      "Epoch 24/150\n",
      "1748/1748 - 0s - loss: 0.4163 - acc: 0.8232\n",
      "Epoch 25/150\n",
      "1748/1748 - 0s - loss: 0.4116 - acc: 0.8221\n",
      "Epoch 26/150\n",
      "1748/1748 - 0s - loss: 0.4054 - acc: 0.8318\n",
      "Epoch 27/150\n",
      "1748/1748 - 0s - loss: 0.4044 - acc: 0.8186\n",
      "Epoch 28/150\n",
      "1748/1748 - 0s - loss: 0.3993 - acc: 0.8186\n",
      "Epoch 29/150\n",
      "1748/1748 - 0s - loss: 0.3968 - acc: 0.8307\n",
      "Epoch 30/150\n",
      "1748/1748 - 0s - loss: 0.3922 - acc: 0.8261\n",
      "Epoch 31/150\n",
      "1748/1748 - 0s - loss: 0.3902 - acc: 0.8284\n",
      "Epoch 32/150\n",
      "1748/1748 - 0s - loss: 0.3863 - acc: 0.8261\n",
      "Epoch 33/150\n",
      "1748/1748 - 0s - loss: 0.3835 - acc: 0.8404\n",
      "Epoch 34/150\n",
      "1748/1748 - 0s - loss: 0.3808 - acc: 0.8364\n",
      "Epoch 35/150\n",
      "1748/1748 - 0s - loss: 0.3793 - acc: 0.8341\n",
      "Epoch 36/150\n",
      "1748/1748 - 0s - loss: 0.3758 - acc: 0.8318\n",
      "Epoch 37/150\n",
      "1748/1748 - 0s - loss: 0.3743 - acc: 0.8450\n",
      "Epoch 38/150\n",
      "1748/1748 - 0s - loss: 0.3728 - acc: 0.8370\n",
      "Epoch 39/150\n",
      "1748/1748 - 0s - loss: 0.3698 - acc: 0.8358\n",
      "Epoch 40/150\n",
      "1748/1748 - 0s - loss: 0.3680 - acc: 0.8392\n",
      "Epoch 41/150\n",
      "1748/1748 - 0s - loss: 0.3660 - acc: 0.8432\n",
      "Epoch 42/150\n",
      "1748/1748 - 0s - loss: 0.3645 - acc: 0.8375\n",
      "Epoch 43/150\n",
      "1748/1748 - 0s - loss: 0.3620 - acc: 0.8410\n",
      "Epoch 44/150\n",
      "1748/1748 - 0s - loss: 0.3602 - acc: 0.8432\n",
      "Epoch 45/150\n",
      "1748/1748 - 0s - loss: 0.3617 - acc: 0.8370\n",
      "Epoch 46/150\n",
      "1748/1748 - 0s - loss: 0.3559 - acc: 0.8518\n",
      "Epoch 47/150\n",
      "1748/1748 - 0s - loss: 0.3551 - acc: 0.8444\n",
      "Epoch 48/150\n",
      "1748/1748 - 0s - loss: 0.3562 - acc: 0.8398\n",
      "Epoch 49/150\n",
      "1748/1748 - 0s - loss: 0.3515 - acc: 0.8467\n",
      "Epoch 50/150\n",
      "1748/1748 - 0s - loss: 0.3503 - acc: 0.8438\n",
      "Epoch 51/150\n",
      "1748/1748 - 0s - loss: 0.3499 - acc: 0.8421\n",
      "Epoch 52/150\n",
      "1748/1748 - 0s - loss: 0.3480 - acc: 0.8530\n",
      "Epoch 53/150\n",
      "1748/1748 - 0s - loss: 0.3462 - acc: 0.8541\n",
      "Epoch 54/150\n",
      "1748/1748 - 0s - loss: 0.3452 - acc: 0.8547\n",
      "Epoch 55/150\n",
      "1748/1748 - 0s - loss: 0.3431 - acc: 0.8558\n",
      "Epoch 56/150\n",
      "1748/1748 - 0s - loss: 0.3434 - acc: 0.8530\n",
      "Epoch 57/150\n",
      "1748/1748 - 0s - loss: 0.3419 - acc: 0.8490\n",
      "Epoch 58/150\n",
      "1748/1748 - 0s - loss: 0.3418 - acc: 0.8507\n",
      "Epoch 59/150\n",
      "1748/1748 - 0s - loss: 0.3429 - acc: 0.8593\n",
      "Epoch 60/150\n",
      "1748/1748 - 0s - loss: 0.3377 - acc: 0.8553\n",
      "Epoch 61/150\n",
      "1748/1748 - 0s - loss: 0.3364 - acc: 0.8535\n",
      "Epoch 62/150\n",
      "1748/1748 - 0s - loss: 0.3377 - acc: 0.8553\n",
      "Epoch 63/150\n",
      "1748/1748 - 0s - loss: 0.3348 - acc: 0.8581\n",
      "Epoch 64/150\n",
      "1748/1748 - 0s - loss: 0.3363 - acc: 0.8518\n",
      "Epoch 65/150\n",
      "1748/1748 - 0s - loss: 0.3326 - acc: 0.8513\n",
      "Epoch 66/150\n",
      "1748/1748 - 0s - loss: 0.3369 - acc: 0.8530\n",
      "Epoch 67/150\n",
      "1748/1748 - 0s - loss: 0.3317 - acc: 0.8558\n",
      "Epoch 68/150\n",
      "1748/1748 - 0s - loss: 0.3297 - acc: 0.8570\n",
      "Epoch 69/150\n",
      "1748/1748 - 0s - loss: 0.3302 - acc: 0.8558\n",
      "Epoch 70/150\n",
      "1748/1748 - 0s - loss: 0.3300 - acc: 0.8576\n",
      "Epoch 71/150\n",
      "1748/1748 - 0s - loss: 0.3303 - acc: 0.8530\n",
      "Epoch 72/150\n",
      "1748/1748 - 0s - loss: 0.3257 - acc: 0.8650\n",
      "Epoch 73/150\n",
      "1748/1748 - 0s - loss: 0.3260 - acc: 0.8587\n",
      "Epoch 74/150\n",
      "1748/1748 - 0s - loss: 0.3248 - acc: 0.8604\n",
      "Epoch 75/150\n",
      "1748/1748 - 0s - loss: 0.3251 - acc: 0.8553\n",
      "Epoch 76/150\n",
      "1748/1748 - 0s - loss: 0.3230 - acc: 0.8604\n",
      "Epoch 77/150\n",
      "1748/1748 - 0s - loss: 0.3218 - acc: 0.8604\n",
      "Epoch 78/150\n",
      "1748/1748 - 0s - loss: 0.3221 - acc: 0.8633\n",
      "Epoch 79/150\n",
      "1748/1748 - 0s - loss: 0.3203 - acc: 0.8593\n",
      "Epoch 80/150\n",
      "1748/1748 - 0s - loss: 0.3223 - acc: 0.8581\n",
      "Epoch 81/150\n",
      "1748/1748 - 0s - loss: 0.3188 - acc: 0.8656\n",
      "Epoch 82/150\n",
      "1748/1748 - 0s - loss: 0.3185 - acc: 0.8621\n",
      "Epoch 83/150\n",
      "1748/1748 - 0s - loss: 0.3165 - acc: 0.8678\n",
      "Epoch 84/150\n",
      "1748/1748 - 0s - loss: 0.3154 - acc: 0.8684\n",
      "Epoch 85/150\n",
      "1748/1748 - 0s - loss: 0.3148 - acc: 0.8684\n",
      "Epoch 86/150\n",
      "1748/1748 - 0s - loss: 0.3149 - acc: 0.8719\n",
      "Epoch 87/150\n",
      "1748/1748 - 0s - loss: 0.3147 - acc: 0.8656\n",
      "Epoch 88/150\n",
      "1748/1748 - 0s - loss: 0.3149 - acc: 0.8604\n",
      "Epoch 89/150\n",
      "1748/1748 - 0s - loss: 0.3104 - acc: 0.8678\n",
      "Epoch 90/150\n",
      "1748/1748 - 0s - loss: 0.3109 - acc: 0.8690\n",
      "Epoch 91/150\n",
      "1748/1748 - 0s - loss: 0.3108 - acc: 0.8713\n",
      "Epoch 92/150\n",
      "1748/1748 - 0s - loss: 0.3088 - acc: 0.8690\n",
      "Epoch 93/150\n",
      "1748/1748 - 0s - loss: 0.3088 - acc: 0.8719\n",
      "Epoch 94/150\n",
      "1748/1748 - 0s - loss: 0.3064 - acc: 0.8724\n",
      "Epoch 95/150\n",
      "1748/1748 - 0s - loss: 0.3096 - acc: 0.8570\n",
      "Epoch 96/150\n",
      "1748/1748 - 0s - loss: 0.3068 - acc: 0.8707\n",
      "Epoch 97/150\n",
      "1748/1748 - 0s - loss: 0.3094 - acc: 0.8633\n",
      "Epoch 98/150\n",
      "1748/1748 - 0s - loss: 0.3071 - acc: 0.8656\n",
      "Epoch 99/150\n",
      "1748/1748 - 0s - loss: 0.3048 - acc: 0.8724\n",
      "Epoch 100/150\n",
      "1748/1748 - 0s - loss: 0.3057 - acc: 0.8713\n",
      "Epoch 101/150\n",
      "1748/1748 - 0s - loss: 0.3038 - acc: 0.8696\n",
      "Epoch 102/150\n",
      "1748/1748 - 0s - loss: 0.3067 - acc: 0.8644\n",
      "Epoch 103/150\n",
      "1748/1748 - 0s - loss: 0.3022 - acc: 0.8724\n",
      "Epoch 104/150\n",
      "1748/1748 - 0s - loss: 0.3022 - acc: 0.8713\n",
      "Epoch 105/150\n",
      "1748/1748 - 0s - loss: 0.3025 - acc: 0.8701\n",
      "Epoch 106/150\n",
      "1748/1748 - 0s - loss: 0.3034 - acc: 0.8593\n",
      "Epoch 107/150\n",
      "1748/1748 - 0s - loss: 0.3024 - acc: 0.8719\n",
      "Epoch 108/150\n",
      "1748/1748 - 0s - loss: 0.3022 - acc: 0.8690\n",
      "Epoch 109/150\n",
      "1748/1748 - 0s - loss: 0.3004 - acc: 0.8747\n",
      "Epoch 110/150\n",
      "1748/1748 - 0s - loss: 0.2987 - acc: 0.8753\n",
      "Epoch 111/150\n",
      "1748/1748 - 0s - loss: 0.2993 - acc: 0.8764\n",
      "Epoch 112/150\n",
      "1748/1748 - 0s - loss: 0.3003 - acc: 0.8724\n",
      "Epoch 113/150\n",
      "1748/1748 - 0s - loss: 0.2985 - acc: 0.8747\n",
      "Epoch 114/150\n",
      "1748/1748 - 0s - loss: 0.2953 - acc: 0.8736\n",
      "Epoch 115/150\n",
      "1748/1748 - 0s - loss: 0.2961 - acc: 0.8701\n",
      "Epoch 116/150\n",
      "1748/1748 - 0s - loss: 0.2961 - acc: 0.8781\n",
      "Epoch 117/150\n",
      "1748/1748 - 0s - loss: 0.2945 - acc: 0.8770\n",
      "Epoch 118/150\n",
      "1748/1748 - 0s - loss: 0.2943 - acc: 0.8741\n",
      "Epoch 119/150\n",
      "1748/1748 - 0s - loss: 0.2962 - acc: 0.8741\n",
      "Epoch 120/150\n",
      "1748/1748 - 0s - loss: 0.2954 - acc: 0.8747\n",
      "Epoch 121/150\n",
      "1748/1748 - 0s - loss: 0.2956 - acc: 0.8741\n",
      "Epoch 122/150\n",
      "1748/1748 - 0s - loss: 0.2925 - acc: 0.8741\n",
      "Epoch 123/150\n",
      "1748/1748 - 0s - loss: 0.2930 - acc: 0.8753\n",
      "Epoch 124/150\n",
      "1748/1748 - 0s - loss: 0.2918 - acc: 0.8770\n",
      "Epoch 125/150\n",
      "1748/1748 - 0s - loss: 0.2915 - acc: 0.8730\n",
      "Epoch 126/150\n",
      "1748/1748 - 0s - loss: 0.2920 - acc: 0.8799\n",
      "Epoch 127/150\n",
      "1748/1748 - 0s - loss: 0.2922 - acc: 0.8759\n",
      "Epoch 128/150\n",
      "1748/1748 - 0s - loss: 0.2897 - acc: 0.8787\n",
      "Epoch 129/150\n",
      "1748/1748 - 0s - loss: 0.2901 - acc: 0.8747\n",
      "Epoch 130/150\n",
      "1748/1748 - 0s - loss: 0.2899 - acc: 0.8753\n",
      "Epoch 131/150\n",
      "1748/1748 - 0s - loss: 0.2889 - acc: 0.8776\n",
      "Epoch 132/150\n",
      "1748/1748 - 0s - loss: 0.2924 - acc: 0.8724\n",
      "Epoch 133/150\n",
      "1748/1748 - 0s - loss: 0.2892 - acc: 0.8822\n",
      "Epoch 134/150\n",
      "1748/1748 - 0s - loss: 0.2893 - acc: 0.8764\n",
      "Epoch 135/150\n",
      "1748/1748 - 0s - loss: 0.2900 - acc: 0.8787\n",
      "Epoch 136/150\n",
      "1748/1748 - 0s - loss: 0.2845 - acc: 0.8827\n",
      "Epoch 137/150\n",
      "1748/1748 - 0s - loss: 0.2879 - acc: 0.8770\n",
      "Epoch 138/150\n",
      "1748/1748 - 0s - loss: 0.2841 - acc: 0.8810\n",
      "Epoch 139/150\n",
      "1748/1748 - 0s - loss: 0.2850 - acc: 0.8844\n",
      "Epoch 140/150\n",
      "1748/1748 - 0s - loss: 0.2840 - acc: 0.8804\n",
      "Epoch 141/150\n",
      "1748/1748 - 0s - loss: 0.2876 - acc: 0.8787\n",
      "Epoch 142/150\n",
      "1748/1748 - 0s - loss: 0.2862 - acc: 0.8787\n",
      "Epoch 143/150\n",
      "1748/1748 - 0s - loss: 0.2840 - acc: 0.8816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/150\n",
      "1748/1748 - 0s - loss: 0.2840 - acc: 0.8856\n",
      "Epoch 145/150\n",
      "1748/1748 - 0s - loss: 0.2830 - acc: 0.8816\n",
      "Epoch 146/150\n",
      "1748/1748 - 0s - loss: 0.2841 - acc: 0.8804\n",
      "Epoch 147/150\n",
      "1748/1748 - 0s - loss: 0.2842 - acc: 0.8793\n",
      "Epoch 148/150\n",
      "1748/1748 - 0s - loss: 0.2840 - acc: 0.8816\n",
      "Epoch 149/150\n",
      "1748/1748 - 0s - loss: 0.2812 - acc: 0.8833\n",
      "Epoch 150/150\n",
      "1748/1748 - 0s - loss: 0.2853 - acc: 0.8856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc710d57b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the tuned model\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(units = 8, activation = 'relu', input_dim = 37))\n",
    "model2.add(Dense(units = 4, activation = 'relu'))\n",
    "model2.add(Dense(units = 3, activation='softmax'))\n",
    "# Compile model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model with test data\n",
    "model2.fit(X_test_scaled,\n",
    "           y_test_categorical,\n",
    "           epochs=150,\n",
    "           verbose = 2\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748/1748 [==============================] - 0s 57us/sample - loss: 0.2794 - acc: 0.8867\n",
      "Testing Data Score: 0.8867276906967163\n"
     ]
    }
   ],
   "source": [
    "# Print the model accuracy\n",
    "print(f\"Testing Data Score: {model2.evaluate(X_test_scaled, y_test_categorical)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is a little bit better than the previous logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model2.save(\"Yanuo_Zhou.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "dev"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
